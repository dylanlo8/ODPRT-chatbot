{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from visual_bge.modeling import Visualized_BGE\n",
    "from pymilvus import (\n",
    "    utility,\n",
    "    CollectionSchema, DataType, FieldSchema, model,\n",
    "    connections, Collection, AnnSearchRequest, RRFRanker,\n",
    ")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = os.getenv('ZILLIS_ENDPOINT')\n",
    "TOKEN = os.getenv('ZILLIS_TOKEN')\n",
    "connections.connect(uri=ENDPOINT, token=TOKEN)\n",
    "\n",
    "COLLECTION_NAME = \"odprt_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO_ID = FieldSchema(\n",
    "    name=\"auto_id\",\n",
    "    dtype=DataType.INT64,\n",
    "    is_primary=True,\n",
    "    auto_id=True\n",
    ")\n",
    "\n",
    "DOC_ID = FieldSchema(\n",
    "    name=\"doc_id\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=500\n",
    ")\n",
    "\n",
    "DOC_SOURCE = FieldSchema(\n",
    "    name=\"doc_source\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=1000,\n",
    "    default_value=\"NA\"\n",
    ")\n",
    "\n",
    "### TEXT FEATURES\n",
    "\n",
    "TEXT = FieldSchema(\n",
    "    name=\"text\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=50000,\n",
    "    default_value=\"\"\n",
    ")\n",
    "\n",
    "TEXT_DENSE_EMBEDDING = FieldSchema(\n",
    "    name=\"text_dense_embedding\",\n",
    "    dtype=DataType.FLOAT_VECTOR,\n",
    "    dim=1024\n",
    ")\n",
    "\n",
    "TEXT_SPARSE_EMBEDDING = FieldSchema(\n",
    "    name=\"text_sparse_embedding\",\n",
    "    dtype=DataType.SPARSE_FLOAT_VECTOR\n",
    ")\n",
    "\n",
    "### IMAGE FEATURES\n",
    "\n",
    "DESCRIPTION = FieldSchema(\n",
    "    name=\"description\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=5000,\n",
    "    default_value=\"\"\n",
    ")\n",
    "\n",
    "DESCRIPTION_EMBEDDING = FieldSchema(\n",
    "    name=\"description_embedding\",\n",
    "    dtype=DataType.FLOAT_VECTOR,\n",
    "    dim=768\n",
    ")\n",
    "\n",
    "IMAGE_EMBEDDING = FieldSchema(\n",
    "    name=\"image_embedding\",\n",
    "    dtype=DataType.FLOAT_VECTOR,\n",
    "    dim=768 # Image embedding dim\n",
    ")\n",
    "\n",
    "### DEFINING THE SCHEMA\n",
    "\n",
    "SCHEMA = CollectionSchema(\n",
    "    fields=[AUTO_ID, DOC_ID, DOC_SOURCE, TEXT, TEXT_DENSE_EMBEDDING, TEXT_SPARSE_EMBEDDING, DESCRIPTION, DESCRIPTION_EMBEDDING, IMAGE_EMBEDDING],\n",
    "    description=\"Schema for indexing documents and images\",\n",
    "    enable_dynamic_field=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'odprt_index' has been dropped\n"
     ]
    }
   ],
   "source": [
    "def create_collection(collection_name, schema):\n",
    "    # Check if the collection exists\n",
    "    if utility.has_collection(collection_name):\n",
    "        print(f\"Collection '{collection_name}' already exists\")\n",
    "        return Collection(name=collection_name) \n",
    "    else:\n",
    "        # Create the collection\n",
    "        return Collection(name=collection_name, schema=schema, using='default', shards_num=2)\n",
    "\n",
    "def drop_collection(collection_name):\n",
    "    # Check if the collection exists\n",
    "    if utility.has_collection(collection_name):\n",
    "        collection = Collection(name=collection_name)\n",
    "        # Release the collection\n",
    "        collection.release()\n",
    "        # Drop the collection if it exists\n",
    "        utility.drop_collection(collection_name)\n",
    "        print(f\"Collection '{collection_name}' has been dropped\")\n",
    "    else:\n",
    "        print(f\"Collection '{collection_name}' does not exist\")\n",
    "\n",
    "drop_collection(COLLECTION_NAME)\n",
    "collection = create_collection(collection_name=COLLECTION_NAME, schema = SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Hyperbolic API Details\n",
    "api_key = os.getenv(\"HYPERBOLIC_API_KEY\")\n",
    "api = \"https://api.hyperbolic.xyz/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Visualized_BGE(\n",
       "  (bge_encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bge_embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (bge_pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (model_visual): CustomCLIP(\n",
       "    (visual): EVAVisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (rope): VisionRotaryEmbeddingFast()\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (inner_attn_ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (rope): VisionRotaryEmbeddingFast()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "            (w2): Linear(in_features=768, out_features=2048, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=2048, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Linear(in_features=768, out_features=512, bias=True)\n",
       "      (patch_dropout): PatchDropout()\n",
       "    )\n",
       "    (text): None\n",
       "  )\n",
       "  (visual_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (cross_entropy): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding Model\n",
    "embedding_model = Visualized_BGE(model_name_bge=\"BAAI/bge-base-en-v1.5\", model_weight=\"Visualized_base_en_v1.5.pth\")\n",
    "embedding_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image\n",
    "image_path = \"image.png\"\n",
    "img = Image.open(image_path)\n",
    "\n",
    "# Encode Images for Payload\n",
    "def encode_image(img):\n",
    "    buffered = BytesIO()\n",
    "    img.save(buffered, format=\"PNG\")\n",
    "    encoded_string = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return encoded_string\n",
    "\n",
    "base64_img = encode_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API payload for generating image summary\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Generate me a summary of this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img}\"},\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    \"model\": \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    \"max_tokens\": 300, # Max length for BGE-Embeddings\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "# Get response from VLM\n",
    "response = requests.post(api, headers=headers, json=payload)\n",
    "\n",
    "# Extract summary from response\n",
    "summary = response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No summary available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1406"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to generate embeddings\n",
    "def generate_embeddings(image_path, text_summary):\n",
    "    \"\"\"Generates embeddings for both image and optional text summary.\"\"\"\n",
    "    description_embedding = []\n",
    "    image_embedding = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if text_summary and text_summary != \"No summary available\":\n",
    "            description_embedding = embedding_model.encode(text=text_summary).tolist()[0]\n",
    "        \n",
    "        # Generate embedding for image\n",
    "        image_embedding = embedding_model.encode(image=image_path).tolist()[0]\n",
    "\n",
    "    return description_embedding, image_embedding\n",
    "\n",
    "# Generate embeddings\n",
    "description_embedding, image_embedding = generate_embeddings(image_path, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"doc_id\": \"doc_001\",\n",
    "        \"doc_source\": \"uploaded_image\",\n",
    "        \"text\": \"\",\n",
    "        \"text_dense_embedding\": [0.0] * 1024,  # Placeholder for text dense embedding\n",
    "        \"text_sparse_embedding\": [],  # Empty sparse vector\n",
    "        \"description\": summary if summary != \"No summary available\" else \"\",\n",
    "        \"description_embedding\": description_embedding,\n",
    "        \"image_embedding\": image_embedding[:768] if len(image_embedding) > 768 else image_embedding  # Ensure correct 768-dim image embedding\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def batch_ingestion(collection, data):\n",
    "    batch_size = 100\n",
    "    total_elements = len(data)  # Ensure batching considers the number of records\n",
    "    total_batches = (total_elements + batch_size - 1) // batch_size\n",
    "\n",
    "    # Using tqdm to create a progress bar\n",
    "    for start in tqdm(range(0, total_elements, batch_size), \n",
    "                      total=total_batches,\n",
    "                      desc=\"Ingesting batches\"):\n",
    "        end = min(start + batch_size, total_elements)\n",
    "        batch = data[start:end]  # Slice batch correctly\n",
    "        collection.insert(batch)  # Insert batch into collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting batches: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_ingestion(collection=collection, data = data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_indexes(collection: Collection) -> None:\n",
    "    # dense embeddings index\n",
    "    collection.create_index(\n",
    "        field_name=\"text_dense_embedding\",\n",
    "        index_params={\n",
    "            \"metric_type\": \"COSINE\",\n",
    "            \"index_type\": \"HNSW\",\n",
    "            \"params\": {\n",
    "                \"M\": 5,\n",
    "                \"efConstruction\": 512\n",
    "            }\n",
    "        },\n",
    "        index_name=\"dense_embeddings_index\"\n",
    "    )\n",
    "    \n",
    "    print(\"Dense embeddings index created\")\n",
    "\n",
    "    # sparse embeddings index\n",
    "    collection.create_index(\n",
    "        field_name=\"text_sparse_embedding\",\n",
    "        index_params={\n",
    "            \"metric_type\": \"IP\",\n",
    "            \"index_type\": \"SPARSE_INVERTED_INDEX\",\n",
    "            \"params\": {\n",
    "                \"drop_ratio_build\": 0.2\n",
    "            }\n",
    "        },\n",
    "        index_name=\"sparse_embeddings_index\"\n",
    "    )\n",
    "\n",
    "    print(\"Sparse embeddings index created\")\n",
    "\n",
    "    # description embeddings index\n",
    "    collection.create_index(\n",
    "        field_name=\"description_embedding\",\n",
    "        index_params={\n",
    "            \"metric_type\": \"COSINE\",\n",
    "            \"index_type\": \"HNSW\"\n",
    "        },\n",
    "        index_name=\"description_embedding_index\"\n",
    "    )\n",
    "    \n",
    "    print(\"description_embedding index created\")\n",
    "\n",
    "    # sparse embeddings index\n",
    "    collection.create_index(\n",
    "        field_name=\"image_embedding\",\n",
    "        index_params={\n",
    "            \"metric_type\": \"COSINE\",\n",
    "            \"index_type\": \"HNSW\",\n",
    "        },\n",
    "        index_name=\"image_embedding_index\"\n",
    "    )\n",
    "    \n",
    "    print(\"image_embedding index created\")\n",
    "    # load\n",
    "    collection.load()\n",
    "    print(\"Collection loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense embeddings index created\n",
      "Sparse embeddings index created\n",
      "description_embedding index created\n",
      "image_embedding index created\n",
      "Collection loaded\n"
     ]
    }
   ],
   "source": [
    "create_all_indexes(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Collection>:\n",
       "-------------\n",
       "<name>: odprt_index\n",
       "<description>: Schema for indexing documents and images\n",
       "<schema>: {'auto_id': True, 'description': 'Schema for indexing documents and images', 'fields': [{'name': 'auto_id', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': True}, {'name': 'doc_id', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 500}}, {'name': 'doc_source', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 1000}, 'default_value': string_data: \"NA\"\n",
       "}, {'name': 'text', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 50000}, 'default_value': string_data: \"\"\n",
       "}, {'name': 'text_dense_embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 1024}}, {'name': 'text_sparse_embedding', 'description': '', 'type': <DataType.SPARSE_FLOAT_VECTOR: 104>}, {'name': 'description', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 5000}, 'default_value': string_data: \"\"\n",
       "}, {'name': 'description_embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 768}}, {'name': 'image_embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 768}}], 'enable_dynamic_field': True}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query: str) -> str:\n",
    "    query_embedding = embedding_model.encode(text=query).tolist()[0]\n",
    "    \n",
    "    search_results = collection.hybrid_search(\n",
    "            reqs=[\n",
    "                AnnSearchRequest(\n",
    "                    data=[query_embedding],  # content vector embedding\n",
    "                    anns_field='description_embedding',  # content vector field\n",
    "                    param={\"metric_type\": \"COSINE\", \"params\": {\"M\": 64, \"efConstruction\": 512}}, \n",
    "                    limit=3\n",
    "                ),\n",
    "                AnnSearchRequest(\n",
    "                    data=[query_embedding],  # keyword vector embedding\n",
    "                    anns_field='image_embedding',  # keyword vector field\n",
    "                    param={\"metric_type\": \"COSINE\", \"params\": {\"M\": 64, \"efConstruction\": 512}}, \n",
    "                    limit=3\n",
    "                )\n",
    "            ],\n",
    "            output_fields=['doc_id', 'text', 'doc_source'],\n",
    "            # using RRFRanker here for reranking\n",
    "            rerank=RRFRanker(),\n",
    "            limit=3\n",
    "            )\n",
    "    \n",
    "    hits = search_results[0]\n",
    "    \n",
    "    context = []\n",
    "    for res in hits:\n",
    "        text = res.text\n",
    "        source = res.doc_source\n",
    "        context.append(f\"Source: {source} \\n Context: {text}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Source: uploaded_image \\n Context: '"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_search(\"Model Architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odprt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
