{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from visual_bge.modeling import Visualized_BGE\n",
    "from pymilvus import (\n",
    "    utility,\n",
    "    CollectionSchema, DataType, FieldSchema, model,\n",
    "    connections, Collection, AnnSearchRequest, RRFRanker,\n",
    ")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = os.getenv('ZILLIS_ENDPOINT')\n",
    "TOKEN = os.getenv('ZILLIS_TOKEN')\n",
    "connections.connect(uri=ENDPOINT, token=TOKEN)\n",
    "\n",
    "COLLECTION_NAME = \"odprt_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO_ID = FieldSchema(\n",
    "    name=\"auto_id\",\n",
    "    dtype=DataType.INT64,\n",
    "    is_primary=True,\n",
    "    auto_id=True\n",
    ")\n",
    "\n",
    "DOC_ID = FieldSchema(\n",
    "    name=\"doc_id\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=500\n",
    ")\n",
    "\n",
    "DOC_SOURCE = FieldSchema(\n",
    "    name=\"doc_source\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=1000,\n",
    "    default_value=\"NA\"\n",
    ")\n",
    "\n",
    "### TEXT FEATURES\n",
    "\n",
    "TEXT = FieldSchema(\n",
    "    name=\"text\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=50000,\n",
    "    default_value=\"\"\n",
    ")\n",
    "\n",
    "TEXT_DENSE_EMBEDDING = FieldSchema(\n",
    "    name=\"text_dense_embedding\",\n",
    "    dtype=DataType.FLOAT_VECTOR,\n",
    "    dim=1024\n",
    ")\n",
    "\n",
    "TEXT_SPARSE_EMBEDDING = FieldSchema(\n",
    "    name=\"text_sparse_embedding\",\n",
    "    dtype=DataType.SPARSE_FLOAT_VECTOR\n",
    ")\n",
    "\n",
    "### IMAGE FEATURES\n",
    "\n",
    "DESCRIPTION = FieldSchema(\n",
    "    name=\"description\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=5000,\n",
    "    default_value=\"\"\n",
    ")\n",
    "\n",
    "DESCRIPTION_EMBEDDING = FieldSchema(\n",
    "    name=\"description_embedding\",\n",
    "    dtype=DataType.FLOAT_VECTOR,\n",
    "    dim=768\n",
    ")\n",
    "\n",
    "IMAGE_EMBEDDING = FieldSchema(\n",
    "    name=\"image_embedding\",\n",
    "    dtype=DataType.FLOAT_VECTOR,\n",
    "    dim=768 # Image embedding dim\n",
    ")\n",
    "\n",
    "### DEFINING THE SCHEMA\n",
    "\n",
    "SCHEMA = CollectionSchema(\n",
    "    fields=[AUTO_ID, DOC_ID, DOC_SOURCE, TEXT, TEXT_DENSE_EMBEDDING, TEXT_SPARSE_EMBEDDING, DESCRIPTION, DESCRIPTION_EMBEDDING, IMAGE_EMBEDDING],\n",
    "    description=\"Schema for indexing documents and images\",\n",
    "    enable_dynamic_field=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'odprt_index' has been dropped\n"
     ]
    }
   ],
   "source": [
    "def create_collection(collection_name, schema):\n",
    "    # Check if the collection exists\n",
    "    if utility.has_collection(collection_name):\n",
    "        print(f\"Collection '{collection_name}' already exists\")\n",
    "        return Collection(name=collection_name) \n",
    "    else:\n",
    "        # Create the collection\n",
    "        return Collection(name=collection_name, schema=schema, using='default', shards_num=2)\n",
    "\n",
    "def drop_collection(collection_name):\n",
    "    # Check if the collection exists\n",
    "    if utility.has_collection(collection_name):\n",
    "        collection = Collection(name=collection_name)\n",
    "        # Release the collection\n",
    "        collection.release()\n",
    "        # Drop the collection if it exists\n",
    "        utility.drop_collection(collection_name)\n",
    "        print(f\"Collection '{collection_name}' has been dropped\")\n",
    "    else:\n",
    "        print(f\"Collection '{collection_name}' does not exist\")\n",
    "\n",
    "drop_collection(COLLECTION_NAME)\n",
    "collection = create_collection(collection_name=COLLECTION_NAME, schema = SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Visualized_BGE(\n",
       "  (bge_encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bge_embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (bge_pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (model_visual): CustomCLIP(\n",
       "    (visual): EVAVisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (rope): VisionRotaryEmbeddingFast()\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (inner_attn_ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (rope): VisionRotaryEmbeddingFast()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "            (w2): Linear(in_features=768, out_features=2048, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=2048, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Linear(in_features=768, out_features=512, bias=True)\n",
       "      (patch_dropout): PatchDropout()\n",
       "    )\n",
       "    (text): None\n",
       "  )\n",
       "  (visual_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (cross_entropy): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding Model\n",
    "embedding_model = Visualized_BGE(model_name_bge=\"BAAI/bge-base-en-v1.5\", model_weight=\"Visualized_base_en_v1.5.pth\")\n",
    "embedding_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chat-58c3912dbeb2452ab369575fe311f4ed', 'object': 'chat.completion', 'created': 1739176612, 'model': 'Qwen/Qwen2-VL-7B-Instruct', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'The image is a line graph titled \"More voters say it \\'really matters\\' who wins the presidency than at any point in the last 20 years.\" The graph shows the percentage of registered voters who believe it really matters who wins the presidential election and those who believe things will be pretty much the same regardless of who is elected, from 2000 to 2020.\\n\\nKey points from the graph:\\n- In 2020, 83% of registered voters believe it really matters who wins the presidential election.\\n- In 2020, 16% of registered voters believe things will be pretty much the same regardless of who is elected.\\n- There is a noticeable increase in the percentage of voters who believe it really matters who wins the presidential election from 2016 to 2020.\\n- The percentage of voters who believe things will be pretty much the same regardless of who is elected has decreased significantly from 2000 to 2020.\\n\\nThe data is based on a telephone survey of U.S. adults conducted from July 23 to August 4, 2020, and is sourced from the Pew Research Center.'}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 30, 'total_tokens': 279, 'completion_tokens': 249}}\n",
      "Summary for sample_images/34.png: The image is a line graph titled \"More voters say it 'really matters' who wins the presidency than at any point in the last 20 years.\" The graph shows the percentage of registered voters who believe it really matters who wins the presidential election and those who believe things will be pretty much the same regardless of who is elected, from 2000 to 2020.\n",
      "\n",
      "Key points from the graph:\n",
      "- In 2020, 83% of registered voters believe it really matters who wins the presidential election.\n",
      "- In 2020, 16% of registered voters believe things will be pretty much the same regardless of who is elected.\n",
      "- There is a noticeable increase in the percentage of voters who believe it really matters who wins the presidential election from 2016 to 2020.\n",
      "- The percentage of voters who believe things will be pretty much the same regardless of who is elected has decreased significantly from 2000 to 2020.\n",
      "\n",
      "The data is based on a telephone survey of U.S. adults conducted from July 23 to August 4, 2020, and is sourced from the Pew Research Center.\n",
      "{'id': 'chat-7d11df1ccf194a549aaa76f4e8778f4c', 'object': 'chat.completion', 'created': 1739176615, 'model': 'Qwen/Qwen2-VL-7B-Instruct', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'The image is a pie chart titled \"Most Latino adults have not heard of the term Latinx; few use it,\" which presents data on awareness and usage of the term \"Latinx\" among U.S. Latino adults. The chart is sourced from a survey conducted by the Pew Research Center from December 3 to 23, 2019.\\n\\nKey points from the chart:\\n- 76% of Latino adults have not heard of the term \"Latinx.\"\\n- 20% of Latino adults do not use the term \"Latinx.\"\\n- 3% of Latino adults use the term \"Latinx.\"\\n\\nThe chart visually represents these percentages with a large brown section labeled \"Have not heard of Latinx,\" a smaller orange section labeled \"Do not use Latinx,\" and an even smaller green section labeled \"Use Latinx.\"'}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 30, 'total_tokens': 203, 'completion_tokens': 173}}\n",
      "Summary for sample_images/43.png: The image is a pie chart titled \"Most Latino adults have not heard of the term Latinx; few use it,\" which presents data on awareness and usage of the term \"Latinx\" among U.S. Latino adults. The chart is sourced from a survey conducted by the Pew Research Center from December 3 to 23, 2019.\n",
      "\n",
      "Key points from the chart:\n",
      "- 76% of Latino adults have not heard of the term \"Latinx.\"\n",
      "- 20% of Latino adults do not use the term \"Latinx.\"\n",
      "- 3% of Latino adults use the term \"Latinx.\"\n",
      "\n",
      "The chart visually represents these percentages with a large brown section labeled \"Have not heard of Latinx,\" a smaller orange section labeled \"Do not use Latinx,\" and an even smaller green section labeled \"Use Latinx.\"\n",
      "{'id': 'chat-9e5cf676f22e4d0eba36ffb84f8900e8', 'object': 'chat.completion', 'created': 1739176618, 'model': 'Qwen/Qwen2-VL-7B-Instruct', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'The image is a pie chart from the Pew Research Center that illustrates the public opinion on the level of spending on policing in the United States as of June 2020. The chart is titled \"Far more Americans favor keeping spending on policing at current levels – or increasing it – than cutting spending.\"\\n\\nHere is a summary of the data:\\n\\n- **42%** of respondents said that spending on policing in their area should stay about the same.\\n- **31%** of respondents said that spending on policing in their area should be increased.\\n- **20%** of respondents said that spending on policing in their area should be a little increased.\\n- **14%** of respondents said that spending on policing in their area should be a little decreased.\\n- **12%** of respondents said that spending on policing in their area should be a lot increased.\\n- **25%** of respondents said that spending on policing in their area should be decreased.\\n\\nThe chart clearly shows that a majority of Americans are either in favor of maintaining or increasing spending on policing, with only a small portion favoring a decrease.'}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 30, 'total_tokens': 259, 'completion_tokens': 229}}\n",
      "Summary for sample_images/46.png: The image is a pie chart from the Pew Research Center that illustrates the public opinion on the level of spending on policing in the United States as of June 2020. The chart is titled \"Far more Americans favor keeping spending on policing at current levels – or increasing it – than cutting spending.\"\n",
      "\n",
      "Here is a summary of the data:\n",
      "\n",
      "- **42%** of respondents said that spending on policing in their area should stay about the same.\n",
      "- **31%** of respondents said that spending on policing in their area should be increased.\n",
      "- **20%** of respondents said that spending on policing in their area should be a little increased.\n",
      "- **14%** of respondents said that spending on policing in their area should be a little decreased.\n",
      "- **12%** of respondents said that spending on policing in their area should be a lot increased.\n",
      "- **25%** of respondents said that spending on policing in their area should be decreased.\n",
      "\n",
      "The chart clearly shows that a majority of Americans are either in favor of maintaining or increasing spending on policing, with only a small portion favoring a decrease.\n",
      "{'id': 'chat-1ff0b7aa602c422ea362a793902eaf22', 'object': 'chat.completion', 'created': 1739176621, 'model': 'Qwen/Qwen2-VL-7B-Instruct', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'The image presents a pie chart from the Pew Research Center that summarizes public opinion on the need for new COVID-19 aid and the urgency of Congress acting on this issue. The chart is based on a survey of the general public conducted from November 12 to 17, 2020.\\n\\nThe pie chart is divided into three main sections:\\n\\n1. **80% Necessary**: This section is the largest, indicating that 80% of the respondents believe another economic assistance package is necessary.\\n2. **As soon as possible, by the current Congress and President Trump**: This section represents 68% of the respondents, suggesting that they want Congress to act on this issue as soon as possible, specifically under the current leadership.\\n3. **After the presidential inauguration in January**: This section is the smallest, with 11% of the respondents suggesting that Congress should wait until after the presidential inauguration in January to consider another aid package.\\n4. **Not necessary**: This section represents 19% of the respondents, indicating that they do not believe another economic assistance package is necessary at this time.\\n\\nThe note at the bottom of the chart indicates that the data is based on the general public and that no answer responses not shown. The source of the survey is the Pew Research Center.'}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 30, 'total_tokens': 295, 'completion_tokens': 265}}\n",
      "Summary for sample_images/160.png: The image presents a pie chart from the Pew Research Center that summarizes public opinion on the need for new COVID-19 aid and the urgency of Congress acting on this issue. The chart is based on a survey of the general public conducted from November 12 to 17, 2020.\n",
      "\n",
      "The pie chart is divided into three main sections:\n",
      "\n",
      "1. **80% Necessary**: This section is the largest, indicating that 80% of the respondents believe another economic assistance package is necessary.\n",
      "2. **As soon as possible, by the current Congress and President Trump**: This section represents 68% of the respondents, suggesting that they want Congress to act on this issue as soon as possible, specifically under the current leadership.\n",
      "3. **After the presidential inauguration in January**: This section is the smallest, with 11% of the respondents suggesting that Congress should wait until after the presidential inauguration in January to consider another aid package.\n",
      "4. **Not necessary**: This section represents 19% of the respondents, indicating that they do not believe another economic assistance package is necessary at this time.\n",
      "\n",
      "The note at the bottom of the chart indicates that the data is based on the general public and that no answer responses not shown. The source of the survey is the Pew Research Center.\n",
      "{'id': 'chat-977bb49f43fd40debacaac497606a069', 'object': 'chat.completion', 'created': 1739176626, 'model': 'Qwen/Qwen2-VL-7B-Instruct', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'The image presents an architectural overview of a system involving RAG (Retrieval-Augmented Generation) workflow, deployment and scaling, analytics, and a legend explaining the color codes used in the diagram.\\n\\n### RAG Workflow\\n1. **User** interacts with a **Chatbot**.\\n2. The **Chatbot** processes the **User Query** and retrieves relevant information from the **Milvus Vector Database**.\\n3. The retrieved information is passed to a **LLM (Large Language Model)** for generating an answer.\\n4. The answer is then provided to the **User**.\\n5. The **Chatbot** also stores the **User Chat History** in a **Firebase Relational Database**.\\n\\n### Deployment and Scaling\\n1. **Users** interact with the system through a **Query Message Queue**.\\n2. The **Load Balancer** distributes queries to **RAG Servers** (Server 1, Server 2, Server 3).\\n3. The **RAG Servers** process the queries and send responses to the **Response Message Queue**.\\n4. The **Users** receive responses from the **Response Message Queue**.\\n\\n### Analytics & Dashboard\\n1. **User** feedback is collected and stored in the **Firebase Relational Database**.\\n2. The **Most Common Queries** are analyzed and refined by human analysts.\\n3. The **Human Analysis and Refinement** process generates insights and updates the system.\\n4. The **Most Common Unanswered Queries'}, 'finish_reason': 'length', 'logprobs': None}], 'usage': {'prompt_tokens': 30, 'total_tokens': 330, 'completion_tokens': 300}}\n",
      "Summary for sample_images/archi.png: The image presents an architectural overview of a system involving RAG (Retrieval-Augmented Generation) workflow, deployment and scaling, analytics, and a legend explaining the color codes used in the diagram.\n",
      "\n",
      "### RAG Workflow\n",
      "1. **User** interacts with a **Chatbot**.\n",
      "2. The **Chatbot** processes the **User Query** and retrieves relevant information from the **Milvus Vector Database**.\n",
      "3. The retrieved information is passed to a **LLM (Large Language Model)** for generating an answer.\n",
      "4. The answer is then provided to the **User**.\n",
      "5. The **Chatbot** also stores the **User Chat History** in a **Firebase Relational Database**.\n",
      "\n",
      "### Deployment and Scaling\n",
      "1. **Users** interact with the system through a **Query Message Queue**.\n",
      "2. The **Load Balancer** distributes queries to **RAG Servers** (Server 1, Server 2, Server 3).\n",
      "3. The **RAG Servers** process the queries and send responses to the **Response Message Queue**.\n",
      "4. The **Users** receive responses from the **Response Message Queue**.\n",
      "\n",
      "### Analytics & Dashboard\n",
      "1. **User** feedback is collected and stored in the **Firebase Relational Database**.\n",
      "2. The **Most Common Queries** are analyzed and refined by human analysts.\n",
      "3. The **Human Analysis and Refinement** process generates insights and updates the system.\n",
      "4. The **Most Common Unanswered Queries\n"
     ]
    }
   ],
   "source": [
    "image_paths = [\"34.png\", \"43.png\", \"46.png\", \"160.png\", \"archi.png\"]\n",
    "image_paths = [f\"sample_images/{path}\" for path in image_paths]\n",
    "\n",
    "# Function to encode an image\n",
    "def encode_image(img):\n",
    "    buffered = BytesIO()\n",
    "    img.save(buffered, format=\"PNG\")\n",
    "    encoded_string = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return encoded_string\n",
    "\n",
    "# Initialise Hyperbolic API Details\n",
    "api_key = os.getenv(\"HYPERBOLIC_API_KEY\")\n",
    "api = \"https://api.hyperbolic.xyz/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "}\n",
    "\n",
    "# List to store summaries\n",
    "summaries = []\n",
    "\n",
    "# Loop through each image path, load the image, encode it, and get the summary\n",
    "for image_path in image_paths:\n",
    "    # Load and encode the image\n",
    "    img = Image.open(image_path)\n",
    "    base64_img = encode_image(img)\n",
    "\n",
    "    # Create the API payload\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Generate me a summary of this image.\"},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img}\"},\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        \"model\": \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "        \"max_tokens\": 300,  # Max length for BGE-Embeddings\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "\n",
    "    # Send the request to the API\n",
    "    response = requests.post(api, headers=headers, json=payload)\n",
    "    print(response.json())\n",
    "    # Extract the summary from the response\n",
    "    summary = response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No summary available\")\n",
    "    summaries.append(summary)\n",
    "\n",
    "    # Print the summary for the current image\n",
    "    print(f\"Summary for {image_path}: {summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to generate embeddings\n",
    "def generate_embeddings(image_path, text_summary):\n",
    "    \"\"\"Generates embeddings for both image and optional text summary.\"\"\"\n",
    "    description_embedding = []\n",
    "    image_embedding = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if text_summary and text_summary != \"No summary available\":\n",
    "            description_embedding = embedding_model.encode(text=text_summary).tolist()[0]\n",
    "        \n",
    "        # Generate embedding for image\n",
    "        image_embedding = embedding_model.encode(image=image_path).tolist()[0]\n",
    "\n",
    "    return description_embedding, image_embedding\n",
    "\n",
    "description_embeddings = []\n",
    "image_embeddings = []\n",
    "\n",
    "# Generate embeddings\n",
    "for i, path in enumerate(image_paths):\n",
    "    description_embedding, image_embedding = generate_embeddings(path, summaries[i])\n",
    "    description_embeddings.append(description_embedding)\n",
    "    image_embeddings.append(image_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"doc_id\": image_paths[i],\n",
    "        \"doc_source\": \"sample_images\",\n",
    "        \"text\": \"\",\n",
    "        \"text_dense_embedding\": [0.0] * 1024,  # Placeholder for text dense embedding\n",
    "        \"text_sparse_embedding\": [],  # Empty sparse vector\n",
    "        \"description\": summaries[i],\n",
    "        \"description_embedding\": description_embeddings[i],  # Placeholder for description embedding\n",
    "        \"image_embedding\": image_embeddings[i],  # Placeholder for image embedding\n",
    "    }\n",
    "    for i in range(len(image_paths))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def batch_ingestion(collection, data):\n",
    "    batch_size = 100\n",
    "    total_elements = len(data)  # Ensure batching considers the number of records\n",
    "    total_batches = (total_elements + batch_size - 1) // batch_size\n",
    "\n",
    "    # Using tqdm to create a progress bar\n",
    "    for start in tqdm(range(0, total_elements, batch_size), \n",
    "                      total=total_batches,\n",
    "                      desc=\"Ingesting batches\"):\n",
    "        end = min(start + batch_size, total_elements)\n",
    "        batch = data[start:end]  # Slice batch correctly\n",
    "        collection.insert(batch)  # Insert batch into collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting batches: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_ingestion(collection=collection, data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_indexes(collection: Collection) -> None:\n",
    "    # dense embeddings index\n",
    "    collection.create_index(\n",
    "        field_name=\"text_dense_embedding\",\n",
    "        index_params={\n",
    "            \"metric_type\": \"COSINE\",\n",
    "            \"index_type\": \"HNSW\",\n",
    "            \"params\": {\n",
    "                \"M\": 5,\n",
    "                \"efConstruction\": 512\n",
    "            }\n",
    "        },\n",
    "        index_name=\"dense_embeddings_index\"\n",
    "    )\n",
    "    \n",
    "    print(\"Dense embeddings index created\")\n",
    "\n",
    "    # sparse embeddings index\n",
    "    collection.create_index(\n",
    "        field_name=\"text_sparse_embedding\",\n",
    "        index_params={\n",
    "            \"metric_type\": \"IP\",\n",
    "            \"index_type\": \"SPARSE_INVERTED_INDEX\",\n",
    "            \"params\": {\n",
    "                \"drop_ratio_build\": 0.2\n",
    "            }\n",
    "        },\n",
    "        index_name=\"sparse_embeddings_index\"\n",
    "    )\n",
    "\n",
    "    print(\"Sparse embeddings index created\")\n",
    "\n",
    "    # description embeddings index\n",
    "    collection.create_index(\n",
    "        field_name=\"description_embedding\",\n",
    "        index_params={\n",
    "            \"metric_type\": \"COSINE\",\n",
    "            \"index_type\": \"HNSW\"\n",
    "        },\n",
    "        index_name=\"description_embedding_index\"\n",
    "    )\n",
    "    \n",
    "    print(\"description_embedding index created\")\n",
    "\n",
    "    # sparse embeddings index\n",
    "    collection.create_index(\n",
    "        field_name=\"image_embedding\",\n",
    "        index_params={\n",
    "            \"metric_type\": \"COSINE\",\n",
    "            \"index_type\": \"HNSW\",\n",
    "        },\n",
    "        index_name=\"image_embedding_index\"\n",
    "    )\n",
    "    \n",
    "    print(\"image_embedding index created\")\n",
    "    # load\n",
    "    collection.load()\n",
    "    print(\"Collection loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense embeddings index created\n",
      "Sparse embeddings index created\n",
      "description_embedding index created\n",
      "image_embedding index created\n",
      "Collection loaded\n"
     ]
    }
   ],
   "source": [
    "create_all_indexes(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query: str) -> str:\n",
    "    query_embedding = embedding_model.encode(text=query).tolist()[0]\n",
    "    \n",
    "    search_results = collection.hybrid_search(\n",
    "            reqs=[\n",
    "                AnnSearchRequest(\n",
    "                    data=[query_embedding],  # content vector embedding\n",
    "                    anns_field='description_embedding',  # content vector field\n",
    "                    param={\"metric_type\": \"COSINE\", \"params\": {\"M\": 64, \"efConstruction\": 512}}, \n",
    "                    limit=3\n",
    "                ),\n",
    "                AnnSearchRequest(\n",
    "                    data=[query_embedding],  # keyword vector embedding\n",
    "                    anns_field='image_embedding',  # keyword vector field\n",
    "                    param={\"metric_type\": \"COSINE\", \"params\": {\"M\": 64, \"efConstruction\": 512}}, \n",
    "                    limit=3\n",
    "                )\n",
    "            ],\n",
    "            output_fields=['doc_id', 'description'],\n",
    "            # using RRFRanker here for reranking\n",
    "            rerank=RRFRanker(),\n",
    "            limit=3\n",
    "            )\n",
    "    \n",
    "    hits = search_results[0]\n",
    "    \n",
    "    context = []\n",
    "    for res in hits:\n",
    "        doc_id = res.doc_id\n",
    "        description = res.description\n",
    "        context.append(f\"Doc_id: {doc_id} \\n Description: {description}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc_id: sample_images/160.png \\n Description: The image presents a pie chart from the Pew Research Center that summarizes public opinion on the need for new COVID-19 aid and the urgency of Congress acting on this issue. The chart is based on a survey of the general public conducted from November 12 to 17, 2020.\\n\\nThe pie chart is divided into three main sections:\\n\\n1. **80% Necessary**: This section is the largest, indicating that 80% of the respondents believe another economic assistance package is necessary.\\n2. **As soon as possible, by the current Congress and President Trump**: This section represents 68% of the respondents, suggesting that they want Congress to act on this issue as soon as possible, specifically under the current leadership.\\n3. **After the presidential inauguration in January**: This section is the smallest, with 11% of the respondents suggesting that Congress should wait until after the presidential inauguration in January to consider another aid package.\\n4. **Not necessary**: This section represents 19% of the respondents, indicating that they do not believe another economic assistance package is necessary at this time.\\n\\nThe note at the bottom of the chart indicates that the data is based on the general public and that no answer responses not shown. The source of the survey is the Pew Research Center.\\n\\nDoc_id: sample_images/43.png \\n Description: The image is a pie chart titled \"Most Latino adults have not heard of the term Latinx; few use it,\" which presents data on awareness and usage of the term \"Latinx\" among U.S. Latino adults. The chart is sourced from a survey conducted by the Pew Research Center from December 3 to 23, 2019.\\n\\nKey points from the chart:\\n- 76% of Latino adults have not heard of the term \"Latinx.\"\\n- 20% of Latino adults do not use the term \"Latinx.\"\\n- 3% of Latino adults use the term \"Latinx.\"\\n\\nThe chart visually represents these percentages with a large brown section labeled \"Have not heard of Latinx,\" a smaller orange section labeled \"Do not use Latinx,\" and an even smaller green section labeled \"Use Latinx.\"\\n\\nDoc_id: sample_images/46.png \\n Description: The image is a pie chart from the Pew Research Center that illustrates the public opinion on the level of spending on policing in the United States as of June 2020. The chart is titled \"Far more Americans favor keeping spending on policing at current levels – or increasing it – than cutting spending.\"\\n\\nHere is a summary of the data:\\n\\n- **42%** of respondents said that spending on policing in their area should stay about the same.\\n- **31%** of respondents said that spending on policing in their area should be increased.\\n- **20%** of respondents said that spending on policing in their area should be a little increased.\\n- **14%** of respondents said that spending on policing in their area should be a little decreased.\\n- **12%** of respondents said that spending on policing in their area should be a lot increased.\\n- **25%** of respondents said that spending on policing in their area should be decreased.\\n\\nThe chart clearly shows that a majority of Americans are either in favor of maintaining or increasing spending on policing, with only a small portion favoring a decrease.'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_search(\"Covid-19\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odprt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
