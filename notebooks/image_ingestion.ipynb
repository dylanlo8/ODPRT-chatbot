{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/odprt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "from chatbot.backend.services.vector_db.db import VectorDB\n",
    "from chatbot.backend.services.models.models import vlm\n",
    "from chatbot.backend.services.models.embedding_model import embedding_model\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [\"archi.png\", \"34.png\"] #\"34.png\", \"43.png\", \"46.png\", \"160.png\", \n",
    "image_paths = [f\"chatbot/backend/sample_images/{path}\" for path in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing vlm\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing vlm\")\n",
    "image_summaries = vlm.generate_image_summaries(useful_image_paths=image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing embedding model\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing embedding model\")\n",
    "dense_embeddings, sparse_embeddings = embedding_model.encode_texts(image_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    image_paths,\n",
    "    [\"sample_images\"] * 2,\n",
    "    image_summaries,\n",
    "    dense_embeddings,\n",
    "    embedding_model.convert_sparse_embeddings(sparse_embeddings),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    'data' : data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ingested successfully\n"
     ]
    }
   ],
   "source": [
    "url = \"http://0.0.0.0:8000/vector-db/insert-documents/\"\n",
    "\n",
    "# Make the POST request to ingest the data\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "# Check the response\n",
    "if response.status_code == 200:\n",
    "    print(\"Data ingested successfully\")\n",
    "else:\n",
    "    print(f\"Failed to ingest data: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://0.0.0.0:8000/vector-db/image-search/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"query\" : \"Model Architecture\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': 'Doc_id: chatbot/backend/sample_images/archi.png \\n Text: The image outlines the architecture of a RAG (Retrieval-Augmented Generation) system. The RAG Workflow involves a user querying a chatbot, which passes the query to a vector database (Milvus) for document ingestion and preprocessing. The database retrieves the most similar chunks/docs and passes them to a Large Language Model (LLM) for generating an answer. The answer is then stored in a Firebase relational database. The Deployment and Scaling section shows users interacting with a load balancer that routes queries to RAG servers. Analytics & Dashboard involves gathering user feedback, analyzing queries, and refining the system based on human analysis. The legend categorizes the architecture components into generative steps, functions/APIs, databases, and servers.\\n\\nDoc_id: chatbot/backend/sample_images/archi.png \\n Text: The image outlines the architecture of a RAG (Retrieval-Augmented Generation) system. The RAG Workflow involves a user querying a chatbot, which passes the query to a vector database (Milvus) for document ingestion and preprocessing. The database retrieves the most similar chunks/docs and passes them to a Large Language Model (LLM) for generating an answer. The answer is then stored in a Firebase relational database. The Deployment and Scaling section shows users interacting with a load balancer that routes queries to RAG servers. Analytics & Dashboard involves gathering user feedback, analyzing queries, and refining the system based on human analysis. The legend categorizes the architecture components into generative steps, functions/APIs, databases, and servers.\\n\\nDoc_id: chatbot/backend/sample_images/34.png \\n Text: The graph shows that more voters believe it really matters who wins the presidency than at any point in the last 20 years. In 2000, 50% of registered voters believed it really mattered who won the presidential election, while 44% believed things would be pretty much the same regardless of who was elected. By 2020, 83% of registered voters believed it really mattered who won the presidential election, while only 16% believed things would be pretty much the same regardless of who was elected.'}\n"
     ]
    }
   ],
   "source": [
    "# Test Hybrid Search\n",
    "response = requests.post(url, json=data)\n",
    "if response.status_code == 200:\n",
    "    print(response.json())\n",
    "else:\n",
    "    print(f\"Failed to hybrid search data: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc_id: chatbot/backend/sample_images/archi.png \\n Text: The image outlines the architecture of a RAG (Retrieval-Augmented Generation) system. The RAG Workflow involves a user querying a chatbot, which passes the query to a vector database (Milvus) for document ingestion and preprocessing. The database retrieves the most similar chunks/docs and passes them to a Large Language Model (LLM) for generating an answer. The answer is then stored in a Firebase relational database. The Deployment and Scaling section shows users interacting with a load balancer that routes queries to RAG servers. Analytics & Dashboard involves gathering user feedback, analyzing queries, and refining the system based on human analysis. The legend categorizes the architecture components into generative steps, functions/APIs, databases, and servers.\\n\\nDoc_id: chatbot/backend/sample_images/archi.png \\n Text: The image outlines the architecture of a RAG (Retrieval-Augmented Generation) system. The RAG Workflow involves a user querying a chatbot, which passes the query to a vector database (Milvus) for document ingestion and preprocessing. The database retrieves the most similar chunks/docs and passes them to a Large Language Model (LLM) for generating an answer. The answer is then stored in a Firebase relational database. The Deployment and Scaling section shows users interacting with a load balancer that routes queries to RAG servers. Analytics & Dashboard involves gathering user feedback, analyzing queries, and refining the system based on human analysis. The legend categorizes the architecture components into generative steps, functions/APIs, databases, and servers.\\n\\nDoc_id: chatbot/backend/sample_images/34.png \\n Text: The graph shows that more voters believe it really matters who wins the presidency than at any point in the last 20 years. In 2000, 50% of registered voters believed it really mattered who won the presidential election, while 44% believed things would be pretty much the same regardless of who was elected. By 2020, 83% of registered voters believed it really mattered who won the presidential election, while only 16% believed things would be pretty much the same regardless of who was elected.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()['context']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odprt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
