{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import tqdm\n",
    "import uuid\n",
    "\n",
    "from FlagEmbedding import FlagModel\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pymilvus import (\n",
    "    utility,\n",
    "    CollectionSchema, DataType, FieldSchema, model,\n",
    "    connections, Collection, AnnSearchRequest, RRFRanker,\n",
    ")\n",
    "from typing import List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = os.getenv('ZILLIS_ENDPOINT')\n",
    "TOKEN = os.getenv('ZILLIS_TOKEN')\n",
    "connections.connect(uri=ENDPOINT, token=TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create milvus collection \n",
    "1. Drop existing collection (if one exists)\n",
    "2. Define Schema -> How documents will be Ingested\n",
    "3. Create Collection with Schema defined in 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"odprt_index\"\n",
    "\n",
    "def drop_collection(collection_name):\n",
    "    # check if the collection exists\n",
    "    if utility.has_collection(collection_name):\n",
    "        collection = Collection(name=collection_name)\n",
    "        # release the collection\n",
    "        collection.release()\n",
    "        # drop the collection if it exists\n",
    "        utility.drop_collection(collection_name)\n",
    "        print(f\"Collection '{collection_name}' has been dropped\")\n",
    "    else:\n",
    "        print(f\"Collection '{collection_name}' does not exist\")\n",
    "\n",
    "# drop_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_id = FieldSchema(\n",
    "    name=\"pk\",\n",
    "    dtype=DataType.INT64,\n",
    "    is_primary=True,\n",
    "    auto_id=True)\n",
    "\n",
    "doc_id = FieldSchema(\n",
    "    name=\"doc_id\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=500\n",
    ")\n",
    "\n",
    "doc_source = FieldSchema(\n",
    "    name=\"doc_source\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=1000,\n",
    "    default_value=\"NA\"\n",
    ")\n",
    "\n",
    "doc_content = FieldSchema(\n",
    "    name=\"text\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=50000,\n",
    "    default_value=\"\"\n",
    ")\n",
    "\n",
    "vec_embeddings = FieldSchema(\n",
    "    name=\"dense_embeddings\",\n",
    "    dtype=DataType.FLOAT_VECTOR,\n",
    "    dim=1024\n",
    ")\n",
    "\n",
    "keyword_embeddings = FieldSchema(\n",
    "    name=\"sparse_embeddings\",\n",
    "    dtype=DataType.SPARSE_FLOAT_VECTOR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = CollectionSchema(\n",
    "  fields=[auto_id, doc_id, doc_content, doc_source, vec_embeddings, keyword_embeddings],\n",
    "  description=\"odprt_schema\",\n",
    "  enable_dynamic_field=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collection(collection_name, schema):\n",
    "    if utility.has_collection(collection_name):\n",
    "        print(f\"Collection '{collection_name}' already exists\")\n",
    "        return Collection(name=collection_name)\n",
    "    # create the collection\n",
    "    return Collection(name=collection_name, schema=schema, using='default', shards_num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = create_collection(collection_name, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_embed_model = FlagModel(\n",
    "    'BAAI/bge-large-en-v1.5'\n",
    ")\n",
    "splade_embed_model = model.sparse.SpladeEmbeddingFunction(\n",
    "    model_name=\"naver/splade-cocondenser-ensembledistil\",\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding texts (and tables) into respective Embedding Models\n",
    "1. Dense Embeddings with BGE\n",
    "2. Sparse Embeddings with SPLADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense_and_sparse_embeddings(all_texts: List[str]):\n",
    "    dense_embeddings_list = bge_embed_model.encode(all_texts)\n",
    "    sparse_embeddings_list = splade_embed_model.encode_documents(all_texts)\n",
    "    return dense_embeddings_list, sparse_embeddings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_embeddings_list, sparse_embeddings_list = get_dense_and_sparse_embeddings(all_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_ingestion(collection, final_docs):\n",
    "    all_ids, all_texts, all_sources = ...\n",
    "    dense_embeddings_list, sparse_embeddings_list = get_dense_and_sparse_embeddings(all_texts)\n",
    "    \n",
    "    data = [\n",
    "        all_ids,\n",
    "        all_texts,\n",
    "        all_sources,\n",
    "        dense_embeddings_list,\n",
    "        sparse_embeddings_list\n",
    "    ]\n",
    "    batch_size = 100\n",
    "    total_elements = len(data[0])\n",
    "    total_batches = (total_elements + batch_size - 1) // batch_size\n",
    "\n",
    "    # using tqdm to create a progress bar\n",
    "    for start in tqdm(range(0, total_elements, batch_size), \n",
    "                     total=total_batches,\n",
    "                     desc=\"Ingesting batches\"):\n",
    "        end = min(start + batch_size, total_elements)\n",
    "        batch = [sublist[start:end] for sublist in data]\n",
    "        collection.insert(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create index\n",
    "1. Delete existing index\n",
    "2. Craete new index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_indexes(collection: Collection, index_names: List[str]) -> None:\n",
    "    collection.release()\n",
    "    for name in index_names:\n",
    "        collection.drop_index(index_name=name)\n",
    "        print(f\"Index '{name}' has been dropped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_indexes(collection, index_names=[\"sparse_embeddings\", \"dense_embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_indexes(collection: Collection) -> None:\n",
    "    # dense embeddings index\n",
    "    collection.create_index(\n",
    "        field_name=\"dense_embeddings\",\n",
    "        index_params={\n",
    "            \"metric_type\": \"COSINE\",\n",
    "            \"index_type\": \"HNSW\",\n",
    "            \"params\": {\n",
    "                \"M\": 5,\n",
    "                \"efConstruction\": 512\n",
    "            }\n",
    "        },\n",
    "        index_name=\"dense_embeddings_index\"\n",
    "    )\n",
    "    \n",
    "    print(\"Dense embeddings index created\")\n",
    "\n",
    "    # sparse embeddings index\n",
    "    collection.create_index(\n",
    "        field_name=\"sparse_embeddings\",\n",
    "        index_params={\n",
    "            \"metric_type\": \"IP\",\n",
    "            \"index_type\": \"SPARSE_INVERTED_INDEX\",\n",
    "            \"params\": {\n",
    "                \"drop_ratio_build\": 0.2\n",
    "            }\n",
    "        },\n",
    "        index_name=\"sparse_embeddings_index\"\n",
    "    )\n",
    "    \n",
    "    print(\"Sparse embeddings index created\")\n",
    "    # load\n",
    "    collection.load()\n",
    "    print(\"Collection loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid search\n",
    "1. Load in collection\n",
    "2. conduct hybrid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query: str) -> str:\n",
    "    dense_embedding = list(bge_embed_model.encode_queries([query], normalize_embeddings=True)[0])\n",
    "    sparse_embedding = list(splade_embed_model.encode_queries([query]))\n",
    "    \n",
    "    search_results = collection.hybrid_search(\n",
    "            reqs=[\n",
    "                AnnSearchRequest(\n",
    "                    data=[dense_embedding],  # content vector embedding\n",
    "                    anns_field='dense_embeddings',  # content vector field\n",
    "                    param={\"metric_type\": \"COSINE\", \"params\": {\"M\": 64, \"efConstruction\": 512}}, \n",
    "                    limit=3\n",
    "                ),\n",
    "                AnnSearchRequest(\n",
    "                    data=list(sparse_embedding),  # keyword vector embedding\n",
    "                    anns_field='sparse_embeddings',  # keyword vector field\n",
    "                    param={\"metric_type\": \"IP\", \"params\": {\"drop_ratio_build\": 0.2}}, \n",
    "                    limit=3\n",
    "                )\n",
    "            ],\n",
    "            output_fields=['doc_id', 'text', 'doc_source'],\n",
    "            # using RRFRanker here for reranking\n",
    "            rerank=RRFRanker(),\n",
    "            limit=3\n",
    "            )\n",
    "    \n",
    "    hits = search_results[0]\n",
    "    \n",
    "    context = []\n",
    "    for res in hits:\n",
    "        text = res.text\n",
    "        source = res.doc_source\n",
    "        context.append(f\"Source: {source}\\nContext: {text}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odprt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
