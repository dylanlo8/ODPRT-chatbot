{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from chromedriver_py import binary_path\n",
    "import time\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service(executable_path=binary_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "visited_urls = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_social_media_link(link):\n",
    "    social_media_domains = ['facebook.com', 'twitter.com',\n",
    "                            'linkedin.com', 'instagram.com', 'youtube.com']\n",
    "    for domain in social_media_domains:\n",
    "        if domain in link:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    links = set()\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        link = urljoin(url, a_tag['href'])\n",
    "        if not is_social_media_link(link):\n",
    "            links.add(link)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_dynamic_page(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    for element in soup.find_all(['header', 'footer', 'nav']):\n",
    "        element.decompose()\n",
    "\n",
    "    main_content = (soup.find('main') or\n",
    "                    soup.find('article') or soup.find('body'))\n",
    "    if not main_content:\n",
    "        raise ValueError(\"Main content not found on the page.\")\n",
    "\n",
    "    text = main_content.get_text(separator=' ', strip=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_nested_links(start_url, depth=2):\n",
    "    if depth == 0 or start_url in visited_urls:\n",
    "        return\n",
    "\n",
    "    print(f\"Scraping: {start_url}\")\n",
    "    visited_urls.add(start_url)\n",
    "\n",
    "    try:\n",
    "        scraped_text = scrape_dynamic_page(start_url)\n",
    "        print(\"Scraped Content:\")\n",
    "        print(scraped_text)\n",
    "\n",
    "        links = extract_links(start_url)\n",
    "\n",
    "        for link in links:\n",
    "            scrape_nested_links(link, depth - 1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {start_url}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
